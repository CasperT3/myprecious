# This job is intended to start upon first installation, "anchoring" all of the replicationdestination restores to the desired node

# Here, we check for pre-existing volumes, so that later on we can optionally mount them, forcing nodefinder onto the node which already has our volumes:
{{- $pvc_logs := (lookup "v1" "PersistentVolumeClaim" .Release.Namespace "logs" ) }}

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-nodefinder
  labels:
    app.elfhosted.com/role: nodefinder
  annotations:
    helm.sh/hook: pre-install
    helm.sh/hook-weight: "-10"  # Run before all the replicationdestinations
spec:
  selector:
    matchLabels: 
      app.elfhosted.com/role: nodefinder
  template:
    metadata:
      labels: 
        app.elfhosted.com/role: nodefinder
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              {{- if or .Values.zurghobbit.enabled .Values.zurgranger.enabled .Values.zurghalfling.enabled .Values.zurgnazgul.enabled .Values.embyhobbit.enabled .Values.embyranger.enabled .Values.embyhalfling.enabled .Values.embynazgul.enabled .Values.jellyfinhobbit.enabled .Values.jellyfinranger.enabled  .Values.jellyfinhalfling.enabled .Values.jellyfinnazgul.enabled }}
              - key: node-role.elfhosted.com/dedicated
              {{- else }}
              - key: node-role.elfhosted.com/contended
              {{- end }}
                operator: In
                values:
                - "true"
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.elfhosted.com/name
                  operator: In
                  values:
                  - zurg # .. but prefer zurg
              topologyKey: "kubernetes.io/hostname"
              namespaceSelector: {}  # i.e., in the absense of any better signal, pick a node which already has zurg on it
      containers:
      - name: "nodefinder"
        command: [ "/usr/bin/sleep" ]
        args: [ "infinity" ]
        image: ghcr.io/elfhosted/tooling:focal-20240530@sha256:458d1f3b54e9455b5cdad3c341d6853a6fdd75ac3f1120931ca3c09ac4b588de
        {{- if .Values.zurghobbit.enabled  }}
        resources: 
          {{ .Values.zurghobbit.resources   | toYaml | indent 10  | trimPrefix "          " }}
        {{- else if .Values.zurgranger.enabled }}
        resources: 
          {{ .Values.zurgranger.resources   | toYaml | indent 10  | trimPrefix "          " }}
        {{- else if .Values.zurghalfling.enabled }}
        resources: 
          {{ .Values.zurghalfling.resources   | toYaml | indent 10  | trimPrefix "          " }}
        {{- else if .Values.zurgnazgul.enabled }}
        resources: 
          {{ .Values.zurgnazgul.resources   | toYaml | indent 10  | trimPrefix "          " }}
        {{- else }}
        resources:
          requests:
            cpu: "500m"
            memory: 500Mi
          limits:
            cpu: "2"
            memory: 4Gi        
        {{- end }}
        volumeMounts:
        {{- if $pvc_logs }}
        - mountPath: /logs
          name: logs
        {{- end }}
      volumes:
      {{- if $pvc_logs }}
      - name: logs
        persistentVolumeClaim:
          claimName: logs
      {{ end }}